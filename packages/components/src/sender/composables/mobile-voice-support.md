## 移动端语音（STT）调研方案

### 背景
针对移动端浏览器实现语音转文字的需求，尽管 `PC` 端的 `Web Speech API` 提供了语音转文字的功能，但其在移动端的支持情况确实存在碎片化和局限性。虽然 `Chrome for Android` 和 `Safari on iOS` 对 `Web Speech API` 的语音识别部分有“部分支持”，但`Firefox for Android` 目前并不支持语音识别功能。  

即使在支持的浏览器上，`Web Speech API` 的实现也可能依赖于服务器端的识别引擎（例如 `Chrome` 将音频发送到 `Google` 服务器进行处理），并且在 `PWA` 或 `WebView` 中可能存在行为不一致或功能受限的问题。

鉴于此，以下是一些移动端浏览器实现语音转文字的技术调研：

### 1. 云服务提供商的语音转文字API (混合模式)

这是目前实现高准确率和低延迟语音转文字最主流且推荐的方案。客户端负责录音并将音频流发送至云服务API，云服务进行语音识别并将文字结果返回。

*   **优点:**
    *   **高准确率和低延迟:** 大型云服务提供商通常拥有最先进的语音识别模型，能够提供卓越的准确性和极低的延迟，尤其适用于实时语音聊天场景。例如，AssemblyAI的Universal-Streaming API声称在300毫秒内提供不可变的转录结果。Speechmatics提供低于1秒的实时转录，准确率高达90%。
    *   **多语言支持:** 通常支持多种语言和方言。Google Cloud Speech-to-Text 支持125种语言和变体，并使用Chirp模型提高识别能力。
    *   **易于集成:** 提供成熟的SDK和RESTful API，方便前端集成。
    *   **可扩展性:** 能够轻松处理大量并发请求。
    *   **高级功能:** 通常包含说话人识别、内容过滤、自定义词汇等高级功能。
    *   **方案成熟:** 使用已有的国内的各种云服务提供商的语音识别，如 一句话识别、实时语音识别等接口。

*   **推荐方案:**
    *   **Google Cloud Speech-to-Text:** 提供将音频转换为文本转录的功能，并可轻松集成到应用程序中。它利用Google AI技术，通过Chirp等先进模型，在准确性和语言支持方面表现出色，并支持短音频、长音频和流式音频的转录。新用户通常可获得免费额度。
    *   **AssemblyAI:** 专注于为对话式AI应用提供实时的语音转文字API。其Universal-Streaming服务通过WebSocket传输不可变的转录文本，具有亚300毫秒的低延迟和智能断句功能，非常适合语音代理和聊天机器人。
    *   **Deepgram:** 提供高准确率、低延迟的语音转文字API，支持实时和批处理模式，并可部署在云端或自托管。它还提供Voice Agent API，用于构建更自然的对话式AI。
    *   **Speechmatics:** 以其高准确率和低延迟的实时语音转文字能力而闻名，支持55+种语言，并可选择在设备上、本地或云端运行，满足不同的隐私需求。
    *   **客户云**、**aliyun**、**baiduyun** 等。

*   **客户端实现考虑:**
    *   使用`navigator.mediaDevices.getUserMedia()`获取麦克风音频流。
    *   将音频流通过WebSocket发送至选择的云服务API。
    *   接收API返回的实时文本转录结果并显示。

### 2. 客户端/离线部署解决方案 (自部署优先)

如果我们对数据隐私、离线能力或完全自主控制有强烈的需求，可以考虑以下客户端或可自部署的方案。

*   **优点:**
    *   **数据隐私:** 音频数据无需发送到第三方服务器。
    *   **离线工作:** 某些方案支持离线识别。
    *   **完全控制:** 可以根据需求进行深度定制和优化。
    *   **低延迟 (本地处理):** 如果模型运行在客户端，理论上可以实现极低的延迟。

*   **推荐方案:**
    *   **Vosk API (基于CMU PocketSphinx):** Vosk是一个离线开源语音识别工具包，支持20多种语言和方言。它的模型体积小（约50MB），支持连续大词汇量转录，通过流式API提供零延迟响应。Vosk可以从小设备（如树莓派或Android智能手机）扩展到大型集群，并提供了Python, Java, Node.js, C#, C++等多种编程语言的绑定。
        *   **实现方式:** 我们可以在服务器端部署Vosk模型，并通过WebSocket从移动浏览器客户端发送音频流进行识别。虽然它强调“离线”，但“离线”通常指在没有互联网连接的情况下在服务器或本地设备上运行，而不是完全在浏览器内运行大型模型。对于移动Web应用，通常仍需要将音频发送到部署了Vosk模型的服务器。
    *   **OpenAI Whisper (自部署后端):** Whisper是OpenAI开源的语音识别系统，以其高准确率而闻名。虽然它本身是一个模型，但我们可以将其部署在自己的服务器上，然后让移动浏览器通过API调用该后端服务。Faster Whisper是其一个优化版本，适用于Docker部署。
        *   **实现方式:** 搭建一个运行Whisper模型的后端服务（例如使用Python/Flask或Node.js），移动端浏览器通过WebSockets或HTTP请求发送音频数据到该服务进行转录。
    *   **TensorFlow.js (理论上的客户端实现):** TensorFlow.js允许在浏览器中运行机器学习模型。理论上，我们可以训练或加载一个轻量级的语音识别模型在客户端浏览器中直接进行推理。然而，这需要较高的技术门槛和资源投入，包括模型优化、浏览器兼容性等，可能无法直接满足高准确率和低延迟的需求，尤其是对于通用的大词汇量识别。对于特定场景的少量词汇识别可能可行。

### 3. 语音聊天输入特定优化

*   **连续识别 (Continuous Recognition):** Web Speech API的`continuous`属性可以设置为`true`以实现连续识别，但这在移动端浏览器上可能存在问题，例如Safari on iOS上的“连续”功能非常难以处理。对于云服务API，它们通常原生支持流式识别，更适合连续的语音输入。
*   **智能断句 (Intelligent Endpointing):** 像AssemblyAI等服务提供了智能断句功能，可以区分思考停顿和对话完成，这对于流畅的语音聊天体验至关重要。
*   **低延迟:** 选择提供亚秒级延迟的服务（如AssemblyAI、Speechmatics、Deepgram）是确保语音聊天体验流畅的关键。

### 总结与建议

*   **优先考虑：混合模式 (云服务API)**
    对于**高准确率、低延迟、跨平台和语音聊天输入**的场景，强烈推荐采用**云服务提供商的语音转文字API**（如Google Cloud Speech-to-Text, AssemblyAI, Deepgram, Speechmatics，客户云、aliyun 等）。这些服务在性能、可扩展性和维护上具有显著优势，并且多数提供易于集成的JavaScript SDK。我们只需要在客户端捕获音频，并通过WebSocket将数据流式传输到API。

*   **自部署/客户端优先考虑：Vosk API 或 Whisper (自部署后端)**
    如果**数据隐私、离线能力或完全自主控制**是核心要求，并且**可以接受在自己的服务器上进行部署**，那么可以考虑**Vosk API**（针对其服务器端部署和客户端音频流）或**OpenAI Whisper**（自部署后端）。Vosk提供了在边缘设备上运行的能力，而Whisper则以其卓越的准确性著称，但需要自行维护后端服务。

*   **避免：直接依赖Web Speech API在移动端**
    由于Web Speech API在移动端的支持不一致、功能受限且通常依赖于浏览器背后的服务器端实现，不建议将其作为生产环境中移动端语音转文字的首选方案，尤其是在对准确率和延迟有严格要求的语音聊天场景。
